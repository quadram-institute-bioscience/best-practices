---
layout: post
title:  "DADA2 basic workflow"
author: at
categories: [ metabarcoding, dada2 ]
image: assets/images/waves.jpg
featured: false
hidden: false
---

 


This tutorial explains how to process amplicon sequencing data using DADA2,
walking through each step from raw reads to taxonomically classified sequences.

## 1. Initial Setup
First, we organise our input/output paths and identify our sequence files using specific patterns (forward_tag and reverse_tag) that distinguish forward and reverse reads.

## 2. Quality Filtering

Using `filterAndTrim()`, we perform initial quality control on the raw reads. 
Critical parameters include:
- `truncLen`: Length at which to truncate reads (e.g., 240bp for forward, 160bp for reverse)
- `maxEE`: Maximum expected errors allowed (typically 2)
- `truncQ`: Truncates reads at the first instance of a quality score below this value
- `maxN`: Maximum number of N bases allowed (usually 0)

This step removes low-quality sequences and trims reads to a consistent length.

## 3. Error Rate Learning

The `learnErrors()` function creates an error model for the sequencing run. This is crucial because it helps distinguish sequencing errors from real biological variation. The function analyzes how quality scores relate to observed error frequencies in the data.

## 4. Dereplication

Using `derepFastq()`, we combine identical sequences into unique sequences while maintaining abundance information. This significantly reduces computational requirements for subsequent steps.

## 5. Sample Inference

The `dada()` function applies the core DADA2 algorithm. It uses the learned error rates to infer the true biological sequences present in each sample. This step effectively "denoises" the data by:
- Identifying and correcting sequencing errors
- Distinguishing real biological variation from technical artifacts
- Creating high-resolution sequence variants

## 6. Merging Paired Reads

`mergePairs()` aligns and combines forward and reverse reads into single contigs. It:
- Requires overlap between paired reads
- Evaluates if forward and reverse reads are consistent
- Removes pairs that don't merge successfully

## 7. Sequence Table Construction

`makeSequenceTable()` creates a matrix of sequence variants and their abundances across all samples.

## 8. Chimera Removal

`removeBimeraDenovo()` identifies and removes chimeric sequences (artificial sequences formed during PCR). The "consensus" method compares sequences across samples to identify likely chimeras.

## 9. Taxonomy Assignment

`assignTaxonomy()` matches sequences against a reference database (like SILVA) to assign taxonomic classifications. This step:
- Uses a naive Bayesian classifier
- Provides hierarchical classification (Kingdom to Species)
- Assigns confidence values to classifications

## 10. Output Generation and Quality Control

The pipeline generates several important outputs:
- A tracking table showing read counts at each processing step
- A sequence table with abundances of each ASV per sample
- Taxonomic assignments for each ASV
- Optional visualizations of community composition

## General considerations
- Quality filtering parameters should be adjusted based on your sequencing quality
- Error learning requires sufficient sequence depth
- Merging success depends on amplicon length and read length
- Taxonomy assignment accuracy depends on database quality and completeness

## Performance Tracking
It's crucial to monitor read loss at each step. Typical acceptable losses:
- Quality filtering: 10-30%
- Merging: 10-30%
- Chimera removal: 10-40%

Unusually high losses at any step may indicate problems with sequencing quality or pipeline parameters.

## Best Practices
1. Always examine quality profiles before setting filtering parameters
2. Use multithread options when available for larger datasets
3. Keep track of read counts throughout the pipeline
4. Consider your specific amplicon characteristics when setting parameters
5. Validate taxonomic assignments against expected community composition

This pipeline produces high-resolution amplicon sequence variants (ASVs) that can be used for downstream ecological analyses and community profiling. 

```r
# Load necessary libraries
library(dada2)      # For high-resolution sample inference from amplicon data
library(phyloseq)   # For handling and analysis of phylogenetic sequencing data
library(ggplot2)    # For data visualization

# 1. Setup Paths and File Names
# Define the path to the directory containing the raw sequencing files
input_dir <- "/path/to/reads/"

# Define the path to the output directory where results will be saved
output_directory <- "/path/to/output_dir"
dir.create(output_directory, showWarnings = FALSE)  # Create the output directory if it doesn't exist

# Define the suffixes used to identify forward and reverse read files
forward_tag <- "_R1_001.fastq.gz"
reverse_tag <- "_R2_001.fastq.gz"

# Define the path to the SILVA reference database used for taxonomy assignment
silva_db <- "/path/to/silva.fasta.gz"

# 2. List and Filter Reads
# Get the list of all forward and reverse reads based on the defined tags
fnFs <- sort(list.files(input_dir, pattern = forward_tag, full.names = TRUE))
fnRs <- sort(list.files(input_dir, pattern = reverse_tag, full.names = TRUE))

# Extract sample names by removing the forward tag from filenames
sample_names <- sapply(strsplit(basename(fnFs), forward_tag), `[`, 1)

# 3. Quality Filtering and Trimming
# Define file paths for the filtered reads
filtFs <- file.path(output_directory, paste0(sample_names, "_F_filt.fastq.gz"))
filtRs <- file.path(output_directory, paste0(sample_names, "_R_filt.fastq.gz"))

# Perform quality filtering on forward and reverse reads
# truncLen: positions where reads are truncated
# maxN: maximum number of ambiguous bases allowed
# maxEE: maximum expected errors allowance
# truncQ: quality score at which to truncate the read
filter_out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                           truncLen = c(240,160), 
                           maxN = 0, 
                           maxEE = c(2,2), 
                           truncQ = 2, 
                           rm.phix = TRUE, 
                           compress = TRUE, 
                           multithread = TRUE)

# 4. Learn Error Rates
# Learn the error rates from the filtered data
errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)

# 5. Dereplication
# Dereplicate the filtered forward and reverse reads
derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)

# Assign sample names to the dereplicated objects
names(derepFs) <- sample_names
names(derepRs) <- sample_names

# 6. Sample Inference and Denoising
# Apply the DADA algorithm to infer sample composition
dadaFs <- dada(derepFs, err = errF, multithread = TRUE)
dadaRs <- dada(derepRs, err = errR, multithread = TRUE)

# 7. Merging Paired Reads
# Merge the forward and reverse reads based on sample names
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, 
                      verbose = TRUE)

# 8. Construct Sequence Table
# Create a table of unique sequences and their abundances in each sample
seqtab <- makeSequenceTable(mergers)

# 9. Remove Chimeras
# Identify and remove chimeric sequences to reduce artifacts
seqtab_nochim <- removeBimeraDenovo(seqtab, 
                                    method = "consensus", 
                                    multithread = TRUE, 
                                    verbose = TRUE)

# 10. Track Reads Through the Pipeline
# Summarize the number of reads at each step to assess pipeline performance
track <- cbind(filter_out, 
              rowSums(seqtab), 
              rowSums(seqtab_nochim))
colnames(track) <- c("input", "filtered", "denoised", "merged", "nonchim")
rownames(track) <- sample_names
write.csv(track, file = file.path(output_directory, "pipeline_tracking.csv"))

# 11. Assign Taxonomy
# Assign taxonomy to each unique sequence using the SILVA database
taxa <- assignTaxonomy(seqtab_nochim, 
                       silva_db, 
                       multithread = TRUE)

# 12. Save Results
# Save the sequence table and taxonomy assignments for downstream analysis
saveRDS(seqtab_nochim, file = file.path(output_directory, "sequence_table.rds"))
saveRDS(taxa, file = file.path(output_directory, "taxonomy.rds"))

# 13. Generate Count Table
# Optionally, generate a count table in a human-readable format
write.csv(as.data.frame(seqtab_nochim), 
          file = file.path(output_directory, "count_table.csv"))

# 14. Visualization (Optional)
# Create a bar plot of taxonomy composition for each sample
ps <- phyloseq(otu_table(seqtab_nochim, taxa_are_rows = FALSE),
               tax_table(taxa))

pdf(file.path(output_directory, "taxa_barplot.pdf"), width = 10, height = 7)
plot_bar(ps, fill = "Genus") + 
    ggtitle("Taxonomic Composition") +
    theme_minimal()
dev.off()
```