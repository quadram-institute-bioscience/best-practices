---
layout: post
title:  "DADA2 basic workflow"
author: at
categories: [ metabarcoding, dada2 ]
image: https://benjjneb.github.io/dada2/images/DADA2_Logo_Text_noVer_640px.png
featured: false
hidden: false
---

> DADA2 has an [excellent tutorial](https://benjjneb.github.io/dada2/tutorial.html) in their website, but we recommend checking the [reference manual] to see all the parameters of each function

## What is Metabarcoding?

Metabarcoding is a powerful molecular technique used to assess biodiversity by sequencing DNA from environmental samples. 
Unlike traditional methods that rely on visual identification of species, metabarcoding leverages high-throughput DNA sequencing to identify a wide range of organisms present in a sample simultaneously. This approach is particularly useful for studying complex communities, such as microbial ecosystems, soil fauna, aquatic organisms, and more, providing comprehensive insights into biodiversity, community structure, and ecosystem health.
The Goal of Metabarcoding Analysis

The primary objective of metabarcoding analysis is to accurately and efficiently identify the variety of organisms within a given environmental sample. This involves several key steps:

* Sample Collection: Gathering environmental samples (e.g., soil, water, air) that contain DNA from multiple organisms.
* DNA Extraction: Isolating DNA from the collected samples.
* Amplification: Targeting specific genetic markers (barcodes) that are conserved across taxa but variable enough to distinguish between different species.
* Sequencing: Using high-throughput sequencing technologies to generate large volumes of DNA sequence data.
* Data Processing and Analysis: Cleaning and interpreting the sequence data to identify and quantify the organisms present.

### Amplicon Sequence Variants (ASVs)

Amplicon Sequence Variants (ASVs) represent highly resolved, exact sequences recovered from sequencing data. Unlike traditional methods that cluster sequences into Operational Taxonomic Units (OTUs) based on similarity thresholds (commonly 97%), ASVs distinguish sequences at the single-nucleotide level. This high resolution allows for more precise identification of organisms, enabling the detection of subtle diversity patterns and facilitating ecological and evolutionary studies.
How DADA2 Identifies ASVs from Raw Reads

[DADA2](https://benjjneb.github.io/dada2/) (Divisive Amplicon Denoising Algorithm 2) is an advanced bioinformatics tool designed to process raw sequencing data and accurately identify ASVs. 
Here's how DADA2 accomplishes this:

* Error Modeling: DADA2 begins by learning the error rates from the sequencing data. It builds a model that distinguishes true biological sequences from those errors introduced during sequencing.
* Quality Filtering and Trimming: The raw reads are filtered to remove low-quality sequences and trimmed to ensure consistency in read lengths. This step enhances the accuracy of subsequent analyses.
* Dereplication: Identical sequences are grouped together to reduce computational load. Dereplication allows DADA2 to efficiently process data by focusing on unique sequences and their abundances.
* Denoising: Applying the error model, DADA2 corrects sequencing errors, effectively "denoising" the data. This process distinguishes true biological variants from artifacts, resulting in the identification of precise ASVs.
* Merging Paired Reads: For paired-end sequencing data, forward and reverse reads are merged to reconstruct the full-length amplicon sequences. This step ensures higher accuracy and completeness of the ASVs.
* Chimera Removal: Chimeric sequences, which are artificial combinations of different DNA fragments formed during PCR amplification, are identified and removed to prevent misleading results.

Through these steps, DADA2 transforms raw sequencing reads into a set of accurate, high-resolution ASVs that represent the true biodiversity within the sample.

###  1. Initial Setup
First, we organise our input/output paths and identify our sequence files using specific patterns (forward_tag and reverse_tag) that distinguish forward and reverse reads.

### 2. Quality Filtering

Using `filterAndTrim()`, we perform initial quality control on the raw reads. 
Critical parameters include:
- `truncLen`: Length at which to truncate reads (you should determine the ideal parameters inspecting the quality profiles of the reads)
- `maxEE`: Maximum expected errors allowed (typically 2)
- `truncQ`: Truncates reads at the first instance of a quality score below this value
- `maxN`: Maximum number of N bases allowed (usually 0)

This step removes low-quality sequences and trims reads to a consistent length.

### 3. Error Rate Learning

The `learnErrors()` function creates an error model for the sequencing run. This is crucial because it helps distinguish sequencing errors from real biological variation. The function analyzes how quality scores relate to observed error frequencies in the data.

### 4. Dereplication

Using `derepFastq()`, we combine identical sequences into unique sequences while maintaining abundance information. This significantly reduces computational requirements for subsequent steps.

### 5. Sample Inference

The `dada()` function applies the core DADA2 algorithm. It uses the learned error rates to infer the true biological sequences present in each sample. This step effectively "denoises" the data by:
- Identifying and correcting sequencing errors
- Distinguishing real biological variation from technical artifacts
- Creating high-resolution sequence variants

### 6. Merging Paired Reads

`mergePairs()` aligns and combines forward and reverse reads into single contigs. It:
- Requires overlap between paired reads
- Evaluates if forward and reverse reads are consistent
- Removes pairs that don't merge successfully

### 7. Sequence Table Construction

`makeSequenceTable()` creates a matrix of sequence variants and their abundances across all samples.

### 8. Chimera Removal

`removeBimeraDenovo()` identifies and removes chimeric sequences (artificial sequences formed during PCR). The "consensus" method compares sequences across samples to identify likely chimeras.

### 9. Taxonomy Assignment

`assignTaxonomy()` matches sequences against a reference database (like SILVA) to assign taxonomic classifications. This step:
- Uses a naive Bayesian classifier
- Provides hierarchical classification (Kingdom to Species)
- Assigns confidence values to classifications

### 10. Output Generation and Quality Control

The pipeline generates several important outputs:
- A tracking table showing read counts at each processing step
- A sequence table with abundances of each ASV per sample
- Taxonomic assignments for each ASV
- Optional visualizations of community composition

### General considerations
- Quality filtering parameters should be adjusted based on your sequencing quality
- Error learning requires sufficient sequence depth
- Merging success depends on amplicon length and read length
- Taxonomy assignment accuracy depends on database quality and completeness

### Performance Tracking
It's crucial to monitor read loss at each step. Typical acceptable losses:
- Quality filtering: 10-30%
- Merging: 10-30%
- Chimera removal: 10-40%

Unusually high losses at any step may indicate problems with sequencing quality or pipeline parameters.

### Best Practices
1. Always examine quality profiles before setting filtering parameters
2. Use multithread options when available for larger datasets
3. Keep track of read counts throughout the pipeline
4. Consider your specific amplicon characteristics when setting parameters
5. Validate taxonomic assignments against expected community composition

## Analysis of an amplicon run with DADA2

This pipeline produces high-resolution amplicon sequence variants (ASVs) that can be used for downstream ecological analyses and community profiling. 

```r
# Load necessary libraries
library(dada2)      # For high-resolution sample inference from amplicon data
library(phyloseq)   # For handling and analysis of phylogenetic sequencing data
library(ggplot2)    # For data visualization

# 1. Setup Paths and File Names
# Define the path to the directory containing the raw sequencing files
input_dir <- "/path/to/reads/"

# Define the path to the output directory where results will be saved
output_directory <- "/path/to/output_dir"
dir.create(output_directory, showWarnings = FALSE)  # Create the output directory if it doesn't exist

# Define the suffixes used to identify forward and reverse read files
forward_tag <- "_R1_001.fastq.gz"
reverse_tag <- "_R2_001.fastq.gz"

# Define the path to the SILVA reference database used for taxonomy assignment
silva_db <- "/path/to/silva.fasta.gz"

# 2. List and Filter Reads
# Get the list of all forward and reverse reads based on the defined tags
fnFs <- sort(list.files(input_dir, pattern = forward_tag, full.names = TRUE))
fnRs <- sort(list.files(input_dir, pattern = reverse_tag, full.names = TRUE))

# Extract sample names by removing the forward tag from filenames
sample_names <- sapply(strsplit(basename(fnFs), forward_tag), `[`, 1)

# 3. Quality Filtering and Trimming
# Define file paths for the filtered reads
filtFs <- file.path(output_directory, paste0(sample_names, "_F_filt.fastq.gz"))
filtRs <- file.path(output_directory, paste0(sample_names, "_R_filt.fastq.gz"))

# Perform quality filtering on forward and reverse reads
# truncLen: positions where reads are truncated
# maxN: maximum number of ambiguous bases allowed
# maxEE: maximum expected errors allowance
# truncQ: quality score at which to truncate the read
filter_out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                           truncLen = c(240,160), 
                           maxN = 0, 
                           maxEE = c(2,2), 
                           truncQ = 2, 
                           rm.phix = TRUE, 
                           compress = TRUE, 
                           multithread = TRUE)

# 4. Learn Error Rates
# Learn the error rates from the filtered data
errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)

# 5. Dereplication
# Dereplicate the filtered forward and reverse reads
derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)

# Assign sample names to the dereplicated objects
names(derepFs) <- sample_names
names(derepRs) <- sample_names

# 6. Sample Inference and Denoising
# Apply the DADA algorithm to infer sample composition
dadaFs <- dada(derepFs, err = errF, multithread = TRUE)
dadaRs <- dada(derepRs, err = errR, multithread = TRUE)

# 7. Merging Paired Reads
# Merge the forward and reverse reads based on sample names
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, 
                      verbose = TRUE)

# 8. Construct Sequence Table
# Create a table of unique sequences and their abundances in each sample
seqtab <- makeSequenceTable(mergers)

# 9. Remove Chimeras
# Identify and remove chimeric sequences to reduce artifacts
seqtab_nochim <- removeBimeraDenovo(seqtab, 
                                    method = "consensus", 
                                    multithread = TRUE, 
                                    verbose = TRUE)

# 10. Track Reads Through the Pipeline
# Summarize the number of reads at each step to assess pipeline performance
track <- cbind(filter_out, 
              rowSums(seqtab), 
              rowSums(seqtab_nochim))
colnames(track) <- c("input", "filtered", "denoised", "merged", "nonchim")
rownames(track) <- sample_names
write.csv(track, file = file.path(output_directory, "pipeline_tracking.csv"))

# 11. Assign Taxonomy
# Assign taxonomy to each unique sequence using the SILVA database
taxa <- assignTaxonomy(seqtab_nochim, 
                       silva_db, 
                       multithread = TRUE)

# 12. Save Results
# Save the sequence table and taxonomy assignments for downstream analysis
saveRDS(seqtab_nochim, file = file.path(output_directory, "sequence_table.rds"))
saveRDS(taxa, file = file.path(output_directory, "taxonomy.rds"))

# 13. Generate Count Table
# Optionally, generate a count table in a human-readable format
write.csv(as.data.frame(seqtab_nochim), 
          file = file.path(output_directory, "count_table.csv"))

# 14. Visualization (Optional)
# Create a bar plot of taxonomy composition for each sample
ps <- phyloseq(otu_table(seqtab_nochim, taxa_are_rows = FALSE),
               tax_table(taxa))

pdf(file.path(output_directory, "taxa_barplot.pdf"), width = 10, height = 7)
plot_bar(ps, fill = "Genus") + 
    ggtitle("Taxonomic Composition") +
    theme_minimal()
dev.off()
```

## Effect of binned quality

Some modern Illumina sequencers, such as NovaSeq, will use a limited set of quality scores in the FASTQ files they produce (i.e. quality score binning).
As described in this [issue in DADA2's repository](https://github.com/benjjneb/dada2/issues/1307), it is necessary to use a custom function for error rate modeling 
`loessErrfun_mod()` as shown below:

```r
# Load necessary libraries
library(dada2)      # For high-resolution sample inference from amplicon data
library(phyloseq)   # For handling and analysis of phylogenetic sequencing data
library(ggplot2)    # For data visualization
library(magrittr)   # For pipe operations
library(dplyr)      # For data manipulation

# 1. Setup Paths and File Names
# Define the path to the directory containing the raw sequencing files
input_dir <- "/path/to/reads/"

# Define the path to the output directory where results will be saved
output_directory <- "/path/to/output_dir"
dir.create(output_directory, showWarnings = FALSE)  # Create the output directory if it doesn't exist

# Define the suffixes used to identify forward and reverse read files
forward_tag <- "_R1_001.fastq.gz"
reverse_tag <- "_R2_001.fastq.gz"

# Define the path to the SILVA reference database used for taxonomy assignment
silva_db <- "/path/to/silva.fasta.gz"

# 2. List and Filter Reads
# Get the list of all forward and reverse reads based on the defined tags
fnFs <- sort(list.files(input_dir, pattern = forward_tag, full.names = TRUE))
fnRs <- sort(list.files(input_dir, pattern = reverse_tag, full.names = TRUE))

# Extract sample names by removing the forward tag from filenames
sample_names <- sapply(strsplit(basename(fnFs), forward_tag), `[`, 1)

# 3. Quality Filtering and Trimming
# Define file paths for the filtered reads
filtFs <- file.path(output_directory, paste0(sample_names, "_F_filt.fastq.gz"))
filtRs <- file.path(output_directory, paste0(sample_names, "_R_filt.fastq.gz"))

# Perform quality filtering on forward and reverse reads
# truncLen: positions where reads are truncated
# maxN: maximum number of ambiguous bases allowed
# maxEE: maximum expected errors allowance
# truncQ: quality score at which to truncate the read
filter_out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                           truncLen = c(240,160), 
                           maxN = 0, 
                           maxEE = c(2,2), 
                           truncQ = 2, 
                           rm.phix = TRUE, 
                           compress = TRUE, 
                           multithread = TRUE)

# 4. Custom Error Function Definition
# Define a custom function for error rate modeling
loessErrfun_mod <- function(trans) { 
    qq <- as.numeric(colnames(trans)) 
    est <- matrix(0, nrow=0, ncol=length(qq)) 
    for(nti in c("A","C","G","T")) { 
        for(ntj in c("A","C","G","T")) { 
            if(nti != ntj) { 
                errs <- trans[paste0(nti,"2",ntj),] 
                tot <- colSums(trans[paste0(nti,"2",c("A","C","G","T")),]) 
                rlogp <- log10((errs+1)/tot) # 1 pseudocount for each error
                rlogp[is.infinite(rlogp)] <- NA 
                df <- data.frame(q=qq, errs=errs, tot=tot, rlogp=rlogp) 
                
                # Gulliem Salazar's solution for loess modeling
                mod.lo <- loess(rlogp ~ q, df, weights = log10(tot))
                pred <- predict(mod.lo, qq) 
                maxrli <- max(which(!is.na(pred))) 
                minrli <- min(which(!is.na(pred))) 
                pred[seq_along(pred) > maxrli] <- pred[[maxrli]] 
                pred[seq_along(pred) < minrli] <- pred[[minrli]] 
                est <- rbind(est, 10^pred) 
            } 
        } 
    } 
    # Enforce error rate boundaries
    MAX_ERROR_RATE <- 0.25 
    MIN_ERROR_RATE <- 1e-7 
    est[est > MAX_ERROR_RATE] <- MAX_ERROR_RATE 
    est[est < MIN_ERROR_RATE] <- MIN_ERROR_RATE 
    
    # Enforce monotonicity
    estorig <- est 
    est <- est %>% 
        data.frame() %>% 
        mutate_all(funs(case_when(. < X40 ~ X40, . >= X40 ~ .))) %>% 
        as.matrix() 
    rownames(est) <- rownames(estorig) 
    colnames(est) <- colnames(estorig) 
    
    # Expand the error matrix with self-transition probabilities
    err <- rbind(
        1 - colSums(est[1:3,]), est[1:3,],
        1 - colSums(est[4:6,]), est[5:6,],
        1 - colSums(est[7:9,]), est[9,],
        1 - colSums(est[10:12,]), est[10:12,]
    ) 
    rownames(err) <- paste0(rep(c("A","C","G","T"), each=4), "2", c("A","C","G","T")) 
    colnames(err) <- colnames(trans) 
    
    return(err) 
}

# 5. Learn Error Rates with Custom Function
# Learn the error rates from the filtered data using the custom error function
errF <- learnErrors(filtFs, multithread = TRUE, errorEstimationFunction = loessErrfun_mod, verbose = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE, errorEstimationFunction = loessErrfun_mod, verbose = TRUE)

# 6. Dereplication
# Dereplicate the filtered forward and reverse reads
derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)

# Assign sample names to the dereplicated objects
names(derepFs) <- sample_names
names(derepRs) <- sample_names

# 7. Sample Inference and Denoising
# Apply the DADA algorithm to infer sample composition
dadaFs <- dada(derepFs, err = errF, multithread = TRUE)
dadaRs <- dada(derepRs, err = errR, multithread = TRUE)

# 8. Merging Paired Reads
# Merge the forward and reverse reads based on sample names
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, 
                      verbose = TRUE)

# 9. Construct Sequence Table
# Create a table of unique sequences and their abundances in each sample
seqtab <- makeSequenceTable(mergers)

# 10. Remove Chimeras
# Identify and remove chimeric sequences to reduce artifacts
seqtab_nochim <- removeBimeraDenovo(seqtab, 
                                    method = "consensus", 
                                    multithread = TRUE, 
                                    verbose = TRUE)

# 11. Track Reads Through the Pipeline
# Summarize the number of reads at each step to assess pipeline performance
track <- cbind(filter_out, 
              rowSums(seqtab), 
              rowSums(seqtab_nochim))
colnames(track) <- c("input", "filtered", "denoised", "merged", "nonchim")
rownames(track) <- sample_names
write.csv(track, file = file.path(output_directory, "pipeline_tracking.csv"))

# 12. Assign Taxonomy
# Assign taxonomy to each unique sequence using the SILVA database
taxa <- assignTaxonomy(seqtab_nochim, 
                       silva_db, 
                       multithread = TRUE)

# 13. Save Results
# Save the sequence table and taxonomy assignments for downstream analysis
saveRDS(seqtab_nochim, file = file.path(output_directory, "sequence_table.rds"))
saveRDS(taxa, file = file.path(output_directory, "taxonomy.rds"))

# 14. Generate Count Table
# Optionally, generate a count table in a human-readable format
write.csv(as.data.frame(seqtab_nochim), 
          file = file.path(output_directory, "count_table.csv"))

# 15. Visualization (Optional)
# Create a bar plot of taxonomy composition for each sample
ps <- phyloseq(otu_table(seqtab_nochim, taxa_are_rows = FALSE),
               tax_table(taxa))

pdf(file.path(output_directory, "taxa_barplot.pdf"), width = 10, height = 7)
plot_bar(ps, fill = "Genus") + 
    ggtitle("Taxonomic Composition") +
    theme_minimal()
dev.off()
```
